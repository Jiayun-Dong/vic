{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/jiayun/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/jiayun/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/jiayun/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/jiayun/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/jiayun/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/jiayun/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "'''Visualization of the filters of VGG16, via gradient ascent in input space.\n",
    "This script can run on CPU in a few minutes.\n",
    "Results example: http://i.imgur.com/4nj4KjN.jpg\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.preprocessing.image import save_img\n",
    "from keras.applications import vgg16\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + K.epsilon())\n",
    "    x *= 0.2\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# build the VGG16 network with ImageNet weights\n",
    "model = vgg16.VGG16(weights='imagenet', include_top=False)\n",
    "print('Model loaded.')\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of the generated pictures for each filter.\n",
    "img_width = 128\n",
    "img_height = 128\n",
    "\n",
    "# the name of the layer we want to visualize\n",
    "# (see model definition at keras/applications/vgg16.py)\n",
    "layer_name = 'block5_pool'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "coef = pd.read_csv('model.csv')\n",
    "coef = coef.drop(coef.columns[[0,1]], axis=1)\n",
    "coef = coef.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil = pd.read_csv('filter.csv')\n",
    "fil = fil.drop(fil.columns[0], axis=1)\n",
    "fil = fil.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "img_input = mpimg.imread('/Users/jiayun/Documents/coding/vgg/data/dog128_2.jpg')\n",
    "# print(img_input.shape)\n",
    "img_input = np.reshape(img_input, (1,img_width,img_height,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model 0\n",
      "Current loss value: 6.3467474\n",
      "Current loss value: 559.94366\n",
      "Current loss value: 1163.8842\n",
      "Current loss value: 1773.7316\n",
      "Current loss value: 2400.9453\n",
      "model 0 processed in 15s\n",
      "Processing model 1\n",
      "Current loss value: 9.084745\n",
      "Current loss value: 639.65326\n",
      "Current loss value: 1325.553\n",
      "Current loss value: 2061.45\n",
      "Current loss value: 2771.2717\n",
      "model 1 processed in 15s\n",
      "Processing model 2\n",
      "Current loss value: 8.078772\n",
      "Current loss value: 622.2379\n",
      "Current loss value: 1274.0319\n",
      "Current loss value: 1989.7207\n",
      "Current loss value: 2733.297\n",
      "model 2 processed in 17s\n",
      "Processing model 3\n",
      "Current loss value: 13.287359\n",
      "Current loss value: 715.39233\n",
      "Current loss value: 1481.8386\n",
      "Current loss value: 2268.9924\n",
      "Current loss value: 3103.4785\n",
      "model 3 processed in 15s\n"
     ]
    }
   ],
   "source": [
    "# this is the placeholder for the input images\n",
    "input_img = model.input\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "\n",
    "# store result\n",
    "kept_filters = []\n",
    "id_filters = []\n",
    "\n",
    "for model_index in range(4):\n",
    "    # we visualize 4 models\n",
    "    print('Processing model %d' % model_index)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # read model\n",
    "    thisModel = coef[model_index]\n",
    "    \n",
    "    # we build a loss function that maximizes the activation of the nth model\n",
    "    layer_output = layer_dict[layer_name].output\n",
    "    loss = 0 # initialize\n",
    "    for filter_index in range(len(fil)):\n",
    "        loss -= thisModel[filter_index] * K.mean(layer_output[:, :, :, filter_index])  # \"-=\" more like a cat and \"+=\" more like a dog\n",
    "\n",
    "    # we compute the gradient of the input picture wrt this loss\n",
    "    grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "    # normalization trick: we normalize the gradient\n",
    "    grads = normalize(grads)\n",
    "\n",
    "    # this function returns the loss and grads given the input picture\n",
    "    iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "    # step size for gradient ascent\n",
    "    step = 5\n",
    "\n",
    "    # we start from a gray image with some random noise\n",
    "    input_img_data = img_input\n",
    "    input_img_data = (input_img_data-0.0)\n",
    "\n",
    "    # we run gradient ascent for 40 steps\n",
    "    for i in range(40):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Current loss value:', loss_value)\n",
    "\n",
    "    print('Current loss value:', loss_value)\n",
    "    \n",
    "    img = deprocess_image(input_img_data[0])\n",
    "    kept_filters.append((img, loss_value))\n",
    "    id_filters.append(filter_index)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('model %d processed in %ds' % (model_index, end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(kept_filters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will stich the best 64 filters on a 8 x 8 grid.\n",
    "n = 2\n",
    "# the filters that have the highest loss are assumed to be better-looking.\n",
    "# we will only keep the top 64 filters.\n",
    "#kept_filters.sort(key=lambda x: x[1], reverse=True)\n",
    "#kept_filters = kept_filters[:n * n]\n",
    "\n",
    "# build a black picture with enough space for\n",
    "# our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n",
    "margin = 5\n",
    "width = n * img_width + (n - 1) * margin\n",
    "height = n * img_height + (n - 1) * margin\n",
    "stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "# fill the picture with our saved filters\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        img, loss = kept_filters[i * n + j]\n",
    "        width_margin = (img_width + margin) * i\n",
    "        height_margin = (img_height + margin) * j\n",
    "        stitched_filters[\n",
    "            width_margin: width_margin + img_width,\n",
    "            height_margin: height_margin + img_height, :] = img\n",
    "\n",
    "# save the result to disk\n",
    "save_img('activation.png', stitched_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img_data = img_input\n",
    "input_img_data = (input_img_data-0.0)\n",
    "img_original = deprocess_image(input_img_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will stich the best 64 filters on a 8 x 8 grid.\n",
    "n = 2\n",
    "# the filters that have the highest loss are assumed to be better-looking.\n",
    "# we will only keep the top 64 filters.\n",
    "#kept_filters.sort(key=lambda x: x[1], reverse=True)\n",
    "#kept_filters = kept_filters[:n * n]\n",
    "\n",
    "# build a black picture with enough space for\n",
    "# our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n",
    "margin = 5\n",
    "width = n * img_width + (n - 1) * margin\n",
    "height = n * img_height + (n - 1) * margin\n",
    "stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "# fill the picture with our saved filters\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        img, loss = kept_filters[i * n + j]\n",
    "        img = abs(img - img_original)\n",
    "#         img = img/(img.max() + 0.001)\n",
    "#         img *= 255\n",
    "        width_margin = (img_width + margin) * i\n",
    "        height_margin = (img_height + margin) * j\n",
    "        stitched_filters[\n",
    "            width_margin: width_margin + img_width,\n",
    "            height_margin: height_margin + img_height, :] = img\n",
    "\n",
    "# save the result to disk\n",
    "save_img('activation2.png', stitched_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open('activation2.png').convert('LA')\n",
    "img.save('activation2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compare with input_img_data\n",
    "\n",
    "# # we will stich the best 64 filters on a 8 x 8 grid.\n",
    "# n = 2\n",
    "# # the filters that have the highest loss are assumed to be better-looking.\n",
    "# # we will only keep the top 64 filters.\n",
    "# #kept_filters.sort(key=lambda x: x[1], reverse=True)\n",
    "# #kept_filters = kept_filters[:n * n]\n",
    "\n",
    "# # build a black picture with enough space for\n",
    "# # our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n",
    "# margin = 5\n",
    "# width = n * img_width + (n - 1) * margin\n",
    "# height = n * img_height + (n - 1) * margin\n",
    "# stitched_filters = np.zeros((width, height))\n",
    "\n",
    "# # fill the picture with our saved filters\n",
    "# for i in range(n):\n",
    "#     for j in range(n):\n",
    "#         img, loss = kept_filters[i * n + j]\n",
    "#         img = abs(img - img_original)\n",
    "#         Y = 0.299 * img[:,:,0] + 0.587 * img[:,:,1] + 0.114 * img[:,:,2]\n",
    "#         width_margin = (img_width + margin) * i\n",
    "#         height_margin = (img_height + margin) * j\n",
    "#         stitched_filters[\n",
    "#             width_margin: width_margin + img_width,\n",
    "#             height_margin: height_margin + img_height] = Y\n",
    "\n",
    "# # save the result to disk\n",
    "# plt.imshow(stitched_filters, cmap = plt.get_cmap('gray'))\n",
    "# plt.show()\n",
    "# plt.savefig('activation3.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
